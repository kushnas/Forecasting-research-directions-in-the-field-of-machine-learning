{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_uri</th>\n",
       "      <th>author_uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://semopenalex.org/work/W1999991268</td>\n",
       "      <td>https://semopenalex.org/author/A5088969692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://semopenalex.org/work/W1999991268</td>\n",
       "      <td>https://semopenalex.org/author/A5087247060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://semopenalex.org/work/W1999991268</td>\n",
       "      <td>https://semopenalex.org/author/A5073470215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://semopenalex.org/work/W1999991268</td>\n",
       "      <td>https://semopenalex.org/author/A5080582549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://semopenalex.org/work/W2811066766</td>\n",
       "      <td>https://semopenalex.org/author/A5034686346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686351</th>\n",
       "      <td>https://semopenalex.org/work/W988530116</td>\n",
       "      <td>https://semopenalex.org/author/A5073650940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686352</th>\n",
       "      <td>https://semopenalex.org/work/W988530116</td>\n",
       "      <td>https://semopenalex.org/author/A5038723763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686353</th>\n",
       "      <td>https://semopenalex.org/work/W988530116</td>\n",
       "      <td>https://semopenalex.org/author/A5052163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686354</th>\n",
       "      <td>https://semopenalex.org/work/W988530116</td>\n",
       "      <td>https://semopenalex.org/author/A5069734877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7686355</th>\n",
       "      <td>https://semopenalex.org/work/W99882638</td>\n",
       "      <td>https://semopenalex.org/author/A5008529665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7686356 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         work_uri  \\\n",
       "0        https://semopenalex.org/work/W1999991268   \n",
       "1        https://semopenalex.org/work/W1999991268   \n",
       "2        https://semopenalex.org/work/W1999991268   \n",
       "3        https://semopenalex.org/work/W1999991268   \n",
       "4        https://semopenalex.org/work/W2811066766   \n",
       "...                                           ...   \n",
       "7686351   https://semopenalex.org/work/W988530116   \n",
       "7686352   https://semopenalex.org/work/W988530116   \n",
       "7686353   https://semopenalex.org/work/W988530116   \n",
       "7686354   https://semopenalex.org/work/W988530116   \n",
       "7686355    https://semopenalex.org/work/W99882638   \n",
       "\n",
       "                                         author_uri  \n",
       "0        https://semopenalex.org/author/A5088969692  \n",
       "1        https://semopenalex.org/author/A5087247060  \n",
       "2        https://semopenalex.org/author/A5073470215  \n",
       "3        https://semopenalex.org/author/A5080582549  \n",
       "4        https://semopenalex.org/author/A5034686346  \n",
       "...                                             ...  \n",
       "7686351  https://semopenalex.org/author/A5073650940  \n",
       "7686352  https://semopenalex.org/author/A5038723763  \n",
       "7686353  https://semopenalex.org/author/A5052163600  \n",
       "7686354  https://semopenalex.org/author/A5069734877  \n",
       "7686355  https://semopenalex.org/author/A5008529665  \n",
       "\n",
       "[7686356 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_author = pd.read_csv('work_author_edges.csv')\n",
    "work_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_uri</th>\n",
       "      <th>source_uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://semopenalex.org/work/W147085818</td>\n",
       "      <td>https://semopenalex.org/source/S4306500158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://semopenalex.org/work/W347964780</td>\n",
       "      <td>https://semopenalex.org/source/S4306538904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://semopenalex.org/work/W348245690</td>\n",
       "      <td>https://semopenalex.org/source/S4306528802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://semopenalex.org/work/W2780058318</td>\n",
       "      <td>https://semopenalex.org/source/S4317411217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://semopenalex.org/work/W2787956682</td>\n",
       "      <td>https://semopenalex.org/source/S4317411217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834399</th>\n",
       "      <td>https://semopenalex.org/work/W985117098</td>\n",
       "      <td>https://semopenalex.org/source/S2764598041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834400</th>\n",
       "      <td>https://semopenalex.org/work/W986445632</td>\n",
       "      <td>https://semopenalex.org/source/S4306538822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834401</th>\n",
       "      <td>https://semopenalex.org/work/W986885764</td>\n",
       "      <td>https://semopenalex.org/source/S4306513017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834402</th>\n",
       "      <td>https://semopenalex.org/work/W988530116</td>\n",
       "      <td>https://semopenalex.org/source/S4306545598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834403</th>\n",
       "      <td>https://semopenalex.org/work/W99882638</td>\n",
       "      <td>https://semopenalex.org/source/S4306502865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1834404 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         work_uri  \\\n",
       "0         https://semopenalex.org/work/W147085818   \n",
       "1         https://semopenalex.org/work/W347964780   \n",
       "2         https://semopenalex.org/work/W348245690   \n",
       "3        https://semopenalex.org/work/W2780058318   \n",
       "4        https://semopenalex.org/work/W2787956682   \n",
       "...                                           ...   \n",
       "1834399   https://semopenalex.org/work/W985117098   \n",
       "1834400   https://semopenalex.org/work/W986445632   \n",
       "1834401   https://semopenalex.org/work/W986885764   \n",
       "1834402   https://semopenalex.org/work/W988530116   \n",
       "1834403    https://semopenalex.org/work/W99882638   \n",
       "\n",
       "                                         source_uri  \n",
       "0        https://semopenalex.org/source/S4306500158  \n",
       "1        https://semopenalex.org/source/S4306538904  \n",
       "2        https://semopenalex.org/source/S4306528802  \n",
       "3        https://semopenalex.org/source/S4317411217  \n",
       "4        https://semopenalex.org/source/S4317411217  \n",
       "...                                             ...  \n",
       "1834399  https://semopenalex.org/source/S2764598041  \n",
       "1834400  https://semopenalex.org/source/S4306538822  \n",
       "1834401  https://semopenalex.org/source/S4306513017  \n",
       "1834402  https://semopenalex.org/source/S4306545598  \n",
       "1834403  https://semopenalex.org/source/S4306502865  \n",
       "\n",
       "[1834404 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_source = pd.read_csv('work_source_edges.csv')\n",
    "work_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_uri</th>\n",
       "      <th>topic_uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://semopenalex.org/work/W2988373386</td>\n",
       "      <td>https://semopenalex.org/topic/T14234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://semopenalex.org/work/W2988373386</td>\n",
       "      <td>https://semopenalex.org/topic/T13851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://semopenalex.org/work/W2988373386</td>\n",
       "      <td>https://semopenalex.org/topic/T13643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://semopenalex.org/work/W2988521487</td>\n",
       "      <td>https://semopenalex.org/topic/T11719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://semopenalex.org/work/W2988521487</td>\n",
       "      <td>https://semopenalex.org/topic/T10215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7554633</th>\n",
       "      <td>https://semopenalex.org/work/W4392903834</td>\n",
       "      <td>https://semopenalex.org/topic/T10270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7554634</th>\n",
       "      <td>https://semopenalex.org/work/W4392903836</td>\n",
       "      <td>https://semopenalex.org/topic/T11366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7554635</th>\n",
       "      <td>https://semopenalex.org/work/W4392903836</td>\n",
       "      <td>https://semopenalex.org/topic/T13650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7554636</th>\n",
       "      <td>https://semopenalex.org/work/W4392903848</td>\n",
       "      <td>https://semopenalex.org/topic/T10028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7554637</th>\n",
       "      <td>https://semopenalex.org/work/W4392903848</td>\n",
       "      <td>https://semopenalex.org/topic/T13083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7554638 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         work_uri  \\\n",
       "0        https://semopenalex.org/work/W2988373386   \n",
       "1        https://semopenalex.org/work/W2988373386   \n",
       "2        https://semopenalex.org/work/W2988373386   \n",
       "3        https://semopenalex.org/work/W2988521487   \n",
       "4        https://semopenalex.org/work/W2988521487   \n",
       "...                                           ...   \n",
       "7554633  https://semopenalex.org/work/W4392903834   \n",
       "7554634  https://semopenalex.org/work/W4392903836   \n",
       "7554635  https://semopenalex.org/work/W4392903836   \n",
       "7554636  https://semopenalex.org/work/W4392903848   \n",
       "7554637  https://semopenalex.org/work/W4392903848   \n",
       "\n",
       "                                    topic_uri  \n",
       "0        https://semopenalex.org/topic/T14234  \n",
       "1        https://semopenalex.org/topic/T13851  \n",
       "2        https://semopenalex.org/topic/T13643  \n",
       "3        https://semopenalex.org/topic/T11719  \n",
       "4        https://semopenalex.org/topic/T10215  \n",
       "...                                       ...  \n",
       "7554633  https://semopenalex.org/topic/T10270  \n",
       "7554634  https://semopenalex.org/topic/T11366  \n",
       "7554635  https://semopenalex.org/topic/T13650  \n",
       "7554636  https://semopenalex.org/topic/T10028  \n",
       "7554637  https://semopenalex.org/topic/T13083  \n",
       "\n",
       "[7554638 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_topic = pd.read_csv('work_topic_edges.csv')\n",
    "work_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_uri</th>\n",
       "      <th>name</th>\n",
       "      <th>orcid</th>\n",
       "      <th>cited_by_count</th>\n",
       "      <th>works_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://semopenalex.org/author/A5015541246</td>\n",
       "      <td>G Trombatore</td>\n",
       "      <td></td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://semopenalex.org/author/A5040304246</td>\n",
       "      <td>Yusuf Kurniawan</td>\n",
       "      <td>https://orcid.org/0000-0001-8818-8391</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://semopenalex.org/author/A5060391792</td>\n",
       "      <td>О. Д. Мрачковский</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://semopenalex.org/author/A5086987802</td>\n",
       "      <td>А. В. Добриков</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://semopenalex.org/author/A5021830753</td>\n",
       "      <td>Gerold Hilty</td>\n",
       "      <td></td>\n",
       "      <td>33</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317132</th>\n",
       "      <td>https://semopenalex.org/author/A5023018076</td>\n",
       "      <td>赵伟民</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317133</th>\n",
       "      <td>https://semopenalex.org/author/A5038723763</td>\n",
       "      <td>李瑰贤</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317134</th>\n",
       "      <td>https://semopenalex.org/author/A5052163600</td>\n",
       "      <td>杨春蕾</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317135</th>\n",
       "      <td>https://semopenalex.org/author/A5069734877</td>\n",
       "      <td>牛红</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2317136</th>\n",
       "      <td>https://semopenalex.org/author/A5073650940</td>\n",
       "      <td>胡长军</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2317137 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         author_uri               name  \\\n",
       "0        https://semopenalex.org/author/A5015541246       G Trombatore   \n",
       "1        https://semopenalex.org/author/A5040304246    Yusuf Kurniawan   \n",
       "2        https://semopenalex.org/author/A5060391792  О. Д. Мрачковский   \n",
       "3        https://semopenalex.org/author/A5086987802     А. В. Добриков   \n",
       "4        https://semopenalex.org/author/A5021830753       Gerold Hilty   \n",
       "...                                             ...                ...   \n",
       "2317132  https://semopenalex.org/author/A5023018076                赵伟民   \n",
       "2317133  https://semopenalex.org/author/A5038723763                李瑰贤   \n",
       "2317134  https://semopenalex.org/author/A5052163600                杨春蕾   \n",
       "2317135  https://semopenalex.org/author/A5069734877                 牛红   \n",
       "2317136  https://semopenalex.org/author/A5073650940                胡长军   \n",
       "\n",
       "                                         orcid  cited_by_count  works_count  \n",
       "0                                                           64           30  \n",
       "1        https://orcid.org/0000-0001-8818-8391              16           24  \n",
       "2                                                            0           34  \n",
       "3                                                            0            3  \n",
       "4                                                           33           78  \n",
       "...                                        ...             ...          ...  \n",
       "2317132                                                      0           18  \n",
       "2317133                                                      1           14  \n",
       "2317134                                                      0            2  \n",
       "2317135                                                      0            1  \n",
       "2317136                                                      0            2  \n",
       "\n",
       "[2317137 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors=pd.read_json('authors.json')\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_nlp=pd.read_csv('works_nlp_rake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>work_uri</th>\n",
       "      <th>title</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>cited_by_count</th>\n",
       "      <th>topics</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>collocation_2</th>\n",
       "      <th>collocation_3</th>\n",
       "      <th>collocation_4</th>\n",
       "      <th>concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>https://semopenalex.org/work/W2369094010</td>\n",
       "      <td>Research on the Approach of Integratng Chinese...</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>Statistical Machine Translation and Natural La...</td>\n",
       "      <td>In this paper,we present a stochastic model in...</td>\n",
       "      <td>[('segmentation', 0.4074), ('tagging', 0.3609)...</td>\n",
       "      <td>Research on the Approach of Integratng Chinese...</td>\n",
       "      <td>['word segmentation', 'speech tagging', 'segme...</td>\n",
       "      <td>['chinese word segmentation', 'segmentation sp...</td>\n",
       "      <td>['integrating chinese word segmentation', 'chi...</td>\n",
       "      <td>['chinese word segmentation', 'dynamic program...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1129</td>\n",
       "      <td>https://semopenalex.org/work/W2353225972</td>\n",
       "      <td>On Translation of Long Sentence in the Light o...</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>Statistical Machine Translation and Natural La...</td>\n",
       "      <td>English and Chinese differ greatly in word ord...</td>\n",
       "      <td>[('chinese', 0.3551), ('languages', 0.3282), (...</td>\n",
       "      <td>On Translation of Long Sentence in the Light o...</td>\n",
       "      <td>['chinese english', 'word order', 'chinese dif...</td>\n",
       "      <td>['difference english chinese', 'languages word...</td>\n",
       "      <td>['order difference english chinese', 'word ord...</td>\n",
       "      <td>['word order difference', 'word order', 'long ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1275</td>\n",
       "      <td>https://semopenalex.org/work/W2099247859</td>\n",
       "      <td>Preliminary speaker recognition experiments on...</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>Statistical Machine Translation and Natural La...</td>\n",
       "      <td>The NATO N4 corpus contains speech collected a...</td>\n",
       "      <td>[('corpus', 0.4298), ('utterances', 0.4227), (...</td>\n",
       "      <td>Preliminary speaker recognition experiments on...</td>\n",
       "      <td>['n4 corpus', 'corpus nato', 'speaker recognit...</td>\n",
       "      <td>['nato n4 corpus', 'n4 corpus nato', 'corpus n...</td>\n",
       "      <td>['speaker recognition experiments nato', 'expe...</td>\n",
       "      <td>['speaker recognition system', 'air force rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1942</td>\n",
       "      <td>https://semopenalex.org/work/W2387159602</td>\n",
       "      <td>Discussion on the Work of Terminology Standard...</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>Statistical Machine Translation and Natural La...</td>\n",
       "      <td>In this paper the author elaborates that termi...</td>\n",
       "      <td>[('standardization', 0.5979), ('standards', 0....</td>\n",
       "      <td>Discussion on the Work of Terminology Standard...</td>\n",
       "      <td>['terminology standardization', 'terminology s...</td>\n",
       "      <td>['terminology standardization work', 'terminol...</td>\n",
       "      <td>['terminology standardization work important',...</td>\n",
       "      <td>['terminology standardization']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4737</td>\n",
       "      <td>https://semopenalex.org/work/W2257065842</td>\n",
       "      <td>연산생성문법에서의 생성과 변형 II</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>Statistical Machine Translation and Natural La...</td>\n",
       "      <td>A generative system is an algorithm can be reg...</td>\n",
       "      <td>[('generative', 0.5093), ('generated', 0.4337)...</td>\n",
       "      <td>연산생성문법에서의 생성과 변형 II. A generative system is an...</td>\n",
       "      <td>['generation language', 'definition generation...</td>\n",
       "      <td>['sentence shaumjan concepts', 'generation lan...</td>\n",
       "      <td>['sentence shaumjan concepts constitutive', 's...</td>\n",
       "      <td>['natural language', 'symbol system', 'generat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122095</th>\n",
       "      <td>122095</td>\n",
       "      <td>77095</td>\n",
       "      <td>32392</td>\n",
       "      <td>https://semopenalex.org/work/W2773810518</td>\n",
       "      <td>English Text Construction</td>\n",
       "      <td>2024</td>\n",
       "      <td>14</td>\n",
       "      <td>Statistical Machine Translation and Natural La...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['english text']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122096</th>\n",
       "      <td>122096</td>\n",
       "      <td>77096</td>\n",
       "      <td>147472</td>\n",
       "      <td>https://semopenalex.org/work/W3128612266</td>\n",
       "      <td>Learning to Select External Knowledge With Mul...</td>\n",
       "      <td>2024</td>\n",
       "      <td>15</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>The Track-1 of DSTC9 aims to effectively answe...</td>\n",
       "      <td>[('apis', 0.3802), ('api', 0.3681), ('dialogue...</td>\n",
       "      <td>Learning to Select External Knowledge With Mul...</td>\n",
       "      <td>['response generation', 'leveraging external',...</td>\n",
       "      <td>['response generation api', 'leveraging extern...</td>\n",
       "      <td>['knowledge grounded response generation', 'le...</td>\n",
       "      <td>['external knowledge resource', 'leveraging ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122097</th>\n",
       "      <td>122097</td>\n",
       "      <td>77097</td>\n",
       "      <td>184903</td>\n",
       "      <td>https://semopenalex.org/work/W4361757231</td>\n",
       "      <td>Nonfactoid Question Answering as Query-Focused...</td>\n",
       "      <td>2024</td>\n",
       "      <td>18</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>Nonfactoid question answering (QA) is one of t...</td>\n",
       "      <td>[('summarizer', 0.3932), ('summarization', 0.3...</td>\n",
       "      <td>Nonfactoid Question Answering as Query-Focused...</td>\n",
       "      <td>['summarization graph', 'summarizer gmqs', 'gr...</td>\n",
       "      <td>['summarization graph enhanced', 'relational g...</td>\n",
       "      <td>['query focused summarization graph', 'inferen...</td>\n",
       "      <td>['natural language processing', 'graph attenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122098</th>\n",
       "      <td>122098</td>\n",
       "      <td>77098</td>\n",
       "      <td>124091</td>\n",
       "      <td>https://semopenalex.org/work/W4388979610</td>\n",
       "      <td>RoFormer: Enhanced transformer with Rotary Pos...</td>\n",
       "      <td>2024</td>\n",
       "      <td>31</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>Position encoding has recently been shown to b...</td>\n",
       "      <td>[('attention', 0.4018), ('transformer', 0.3726...</td>\n",
       "      <td>RoFormer: Enhanced transformer with Rotary Pos...</td>\n",
       "      <td>['position embedding', 'position encoding', 'e...</td>\n",
       "      <td>['embedding position encoding', 'transformer b...</td>\n",
       "      <td>['attention relative position encoding', 'tran...</td>\n",
       "      <td>['long text classification', 'relative positio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122099</th>\n",
       "      <td>122099</td>\n",
       "      <td>77099</td>\n",
       "      <td>59702</td>\n",
       "      <td>https://semopenalex.org/work/W2773052985</td>\n",
       "      <td>Low-Resource Named Entity Recognition with Cro...</td>\n",
       "      <td>2024</td>\n",
       "      <td>42</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "      <td>Low-resource named entity recognition is still...</td>\n",
       "      <td>[('nlp', 0.37), ('crf', 0.366), ('entity', 0.3...</td>\n",
       "      <td>Low-Resource Named Entity Recognition with Cro...</td>\n",
       "      <td>['entity recognition', 'neural crfs', 'transfe...</td>\n",
       "      <td>['entity recognition cross', 'named entity rec...</td>\n",
       "      <td>['entity recognition cross lingual', 'named en...</td>\n",
       "      <td>['named entity recognition', 'conditional rand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122100 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
       "0                  0             0          33   \n",
       "1                  1             1        1129   \n",
       "2                  2             2        1275   \n",
       "3                  3             3        1942   \n",
       "4                  4             4        4737   \n",
       "...              ...           ...         ...   \n",
       "122095        122095         77095       32392   \n",
       "122096        122096         77096      147472   \n",
       "122097        122097         77097      184903   \n",
       "122098        122098         77098      124091   \n",
       "122099        122099         77099       59702   \n",
       "\n",
       "                                        work_uri  \\\n",
       "0       https://semopenalex.org/work/W2369094010   \n",
       "1       https://semopenalex.org/work/W2353225972   \n",
       "2       https://semopenalex.org/work/W2099247859   \n",
       "3       https://semopenalex.org/work/W2387159602   \n",
       "4       https://semopenalex.org/work/W2257065842   \n",
       "...                                          ...   \n",
       "122095  https://semopenalex.org/work/W2773810518   \n",
       "122096  https://semopenalex.org/work/W3128612266   \n",
       "122097  https://semopenalex.org/work/W4361757231   \n",
       "122098  https://semopenalex.org/work/W4388979610   \n",
       "122099  https://semopenalex.org/work/W2773052985   \n",
       "\n",
       "                                                    title  publication_year  \\\n",
       "0       Research on the Approach of Integratng Chinese...              2001   \n",
       "1       On Translation of Long Sentence in the Light o...              2001   \n",
       "2       Preliminary speaker recognition experiments on...              2001   \n",
       "3       Discussion on the Work of Terminology Standard...              2001   \n",
       "4                                     연산생성문법에서의 생성과 변형 II              2001   \n",
       "...                                                   ...               ...   \n",
       "122095                          English Text Construction              2024   \n",
       "122096  Learning to Select External Knowledge With Mul...              2024   \n",
       "122097  Nonfactoid Question Answering as Query-Focused...              2024   \n",
       "122098  RoFormer: Enhanced transformer with Rotary Pos...              2024   \n",
       "122099  Low-Resource Named Entity Recognition with Cro...              2024   \n",
       "\n",
       "        cited_by_count                                             topics  \\\n",
       "0                    0  Statistical Machine Translation and Natural La...   \n",
       "1                    0  Statistical Machine Translation and Natural La...   \n",
       "2                    0  Statistical Machine Translation and Natural La...   \n",
       "3                    0  Statistical Machine Translation and Natural La...   \n",
       "4                    0  Statistical Machine Translation and Natural La...   \n",
       "...                ...                                                ...   \n",
       "122095              14  Statistical Machine Translation and Natural La...   \n",
       "122096              15                        Natural Language Processing   \n",
       "122097              18                        Natural Language Processing   \n",
       "122098              31                        Natural Language Processing   \n",
       "122099              42                        Natural Language Processing   \n",
       "\n",
       "                                                 abstract  \\\n",
       "0       In this paper,we present a stochastic model in...   \n",
       "1       English and Chinese differ greatly in word ord...   \n",
       "2       The NATO N4 corpus contains speech collected a...   \n",
       "3       In this paper the author elaborates that termi...   \n",
       "4       A generative system is an algorithm can be reg...   \n",
       "...                                                   ...   \n",
       "122095                                                NaN   \n",
       "122096  The Track-1 of DSTC9 aims to effectively answe...   \n",
       "122097  Nonfactoid question answering (QA) is one of t...   \n",
       "122098  Position encoding has recently been shown to b...   \n",
       "122099  Low-resource named entity recognition is still...   \n",
       "\n",
       "                                                 keywords  \\\n",
       "0       [('segmentation', 0.4074), ('tagging', 0.3609)...   \n",
       "1       [('chinese', 0.3551), ('languages', 0.3282), (...   \n",
       "2       [('corpus', 0.4298), ('utterances', 0.4227), (...   \n",
       "3       [('standardization', 0.5979), ('standards', 0....   \n",
       "4       [('generative', 0.5093), ('generated', 0.4337)...   \n",
       "...                                                   ...   \n",
       "122095                                                 []   \n",
       "122096  [('apis', 0.3802), ('api', 0.3681), ('dialogue...   \n",
       "122097  [('summarizer', 0.3932), ('summarization', 0.3...   \n",
       "122098  [('attention', 0.4018), ('transformer', 0.3726...   \n",
       "122099  [('nlp', 0.37), ('crf', 0.366), ('entity', 0.3...   \n",
       "\n",
       "                                            combined_text  \\\n",
       "0       Research on the Approach of Integratng Chinese...   \n",
       "1       On Translation of Long Sentence in the Light o...   \n",
       "2       Preliminary speaker recognition experiments on...   \n",
       "3       Discussion on the Work of Terminology Standard...   \n",
       "4       연산생성문법에서의 생성과 변형 II. A generative system is an...   \n",
       "...                                                   ...   \n",
       "122095                                                NaN   \n",
       "122096  Learning to Select External Knowledge With Mul...   \n",
       "122097  Nonfactoid Question Answering as Query-Focused...   \n",
       "122098  RoFormer: Enhanced transformer with Rotary Pos...   \n",
       "122099  Low-Resource Named Entity Recognition with Cro...   \n",
       "\n",
       "                                            collocation_2  \\\n",
       "0       ['word segmentation', 'speech tagging', 'segme...   \n",
       "1       ['chinese english', 'word order', 'chinese dif...   \n",
       "2       ['n4 corpus', 'corpus nato', 'speaker recognit...   \n",
       "3       ['terminology standardization', 'terminology s...   \n",
       "4       ['generation language', 'definition generation...   \n",
       "...                                                   ...   \n",
       "122095                                                 []   \n",
       "122096  ['response generation', 'leveraging external',...   \n",
       "122097  ['summarization graph', 'summarizer gmqs', 'gr...   \n",
       "122098  ['position embedding', 'position encoding', 'e...   \n",
       "122099  ['entity recognition', 'neural crfs', 'transfe...   \n",
       "\n",
       "                                            collocation_3  \\\n",
       "0       ['chinese word segmentation', 'segmentation sp...   \n",
       "1       ['difference english chinese', 'languages word...   \n",
       "2       ['nato n4 corpus', 'n4 corpus nato', 'corpus n...   \n",
       "3       ['terminology standardization work', 'terminol...   \n",
       "4       ['sentence shaumjan concepts', 'generation lan...   \n",
       "...                                                   ...   \n",
       "122095                                                 []   \n",
       "122096  ['response generation api', 'leveraging extern...   \n",
       "122097  ['summarization graph enhanced', 'relational g...   \n",
       "122098  ['embedding position encoding', 'transformer b...   \n",
       "122099  ['entity recognition cross', 'named entity rec...   \n",
       "\n",
       "                                            collocation_4  \\\n",
       "0       ['integrating chinese word segmentation', 'chi...   \n",
       "1       ['order difference english chinese', 'word ord...   \n",
       "2       ['speaker recognition experiments nato', 'expe...   \n",
       "3       ['terminology standardization work important',...   \n",
       "4       ['sentence shaumjan concepts constitutive', 's...   \n",
       "...                                                   ...   \n",
       "122095                                                 []   \n",
       "122096  ['knowledge grounded response generation', 'le...   \n",
       "122097  ['query focused summarization graph', 'inferen...   \n",
       "122098  ['attention relative position encoding', 'tran...   \n",
       "122099  ['entity recognition cross lingual', 'named en...   \n",
       "\n",
       "                                                 concepts  \n",
       "0       ['chinese word segmentation', 'dynamic program...  \n",
       "1       ['word order difference', 'word order', 'long ...  \n",
       "2       ['speaker recognition system', 'air force rese...  \n",
       "3                         ['terminology standardization']  \n",
       "4       ['natural language', 'symbol system', 'generat...  \n",
       "...                                                   ...  \n",
       "122095                                   ['english text']  \n",
       "122096  ['external knowledge resource', 'leveraging ex...  \n",
       "122097  ['natural language processing', 'graph attenti...  \n",
       "122098  ['long text classification', 'relative positio...  \n",
       "122099  ['named entity recognition', 'conditional rand...  \n",
       "\n",
       "[122100 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "works_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объединенный файл full_data.csv успешно создан\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Загрузка всех данных\n",
    "work_author = pd.read_csv('work_author_edges.csv')\n",
    "work_source = pd.read_csv('work_source_edges.csv')\n",
    "work_topic = pd.read_csv('work_topic_edges.csv')\n",
    "authors = pd.read_json('authors.json')\n",
    "works_nlp = pd.read_csv('works_nlp_with_collac.csv')  # Предполагается, что это основной файл с работами\n",
    "\n",
    "# Объединение данных\n",
    "# 1. Сначала объединим информацию об авторах\n",
    "authors_expanded = work_author.merge(authors, on='author_uri', how='left')\n",
    "\n",
    "# 2. Агрегируем данные по работам (количество авторов, суммарные показатели авторов)\n",
    "author_stats = authors_expanded.groupby('work_uri').agg({\n",
    "    'author_uri': 'count',\n",
    "    'cited_by_count': 'sum',\n",
    "    'works_count': 'sum'\n",
    "}).rename(columns={\n",
    "    'author_uri': 'num_authors',\n",
    "    'cited_by_count': 'total_author_citations',\n",
    "    'works_count': 'total_author_works'\n",
    "}).reset_index()\n",
    "\n",
    "# 3. Объединим информацию об источниках\n",
    "source_stats = work_source.groupby('work_uri').agg({\n",
    "    'source_uri': 'count'\n",
    "}).rename(columns={\n",
    "    'source_uri': 'num_sources'\n",
    "}).reset_index()\n",
    "\n",
    "# 4. Объединим информацию о темах\n",
    "topic_stats = work_topic.groupby('work_uri').agg({\n",
    "    'topic_uri': 'count'\n",
    "}).rename(columns={\n",
    "    'topic_uri': 'num_topics'\n",
    "}).reset_index()\n",
    "\n",
    "# 5. Объединим все с основным датасетом работ\n",
    "full_data = works_nlp.merge(author_stats, on='work_uri', how='left')\n",
    "full_data = full_data.merge(source_stats, on='work_uri', how='left')\n",
    "full_data = full_data.merge(topic_stats, on='work_uri', how='left')\n",
    "\n",
    "# Заполним пропущенные значения\n",
    "full_data.fillna({\n",
    "    'num_authors': 0,\n",
    "    'total_author_citations': 0,\n",
    "    'total_author_works': 0,\n",
    "    'num_sources': 0,\n",
    "    'num_topics': 0\n",
    "}, inplace=True)\n",
    "\n",
    "# Сохраним объединенный файл\n",
    "full_data.to_csv('full_data.csv', index=False)\n",
    "print(\"Объединенный файл full_data.csv успешно создан\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Моя замечательная модель с кривыми обучения\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import (train_test_split, TimeSeriesSplit, StratifiedKFold)\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                           f1_score, roc_auc_score, roc_curve, confusion_matrix)\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnhancedConceptPredictor:\n",
    "    def __init__(self, full_data_path, sample_size=None, model_type='xgb',\n",
    "                min_collocation2=5, min_collocation3=3):\n",
    "        \"\"\"\n",
    "        Инициализация с настраиваемыми параметрами фильтрации коллокаций\n",
    "\n",
    "        Parameters:\n",
    "            full_data_path: путь к данным\n",
    "            sample_size: размер выборки\n",
    "            model_type: тип модели\n",
    "            min_collocation2: минимальное количество статей для collocation_2 (по умолчанию 5)\n",
    "            min_collocation3: минимальное количество статей для collocation_3 (по умолчанию 3)\n",
    "        \"\"\"\n",
    "        self.full_data_path = full_data_path\n",
    "        self.sample_size = sample_size\n",
    "        self.model_type = model_type.lower()\n",
    "        self.best_params = None\n",
    "        self.current_year = datetime.now().year\n",
    "        self.work_data = None\n",
    "        self.min_collocation2 = min_collocation2  # Минимум статей для collocation_2\n",
    "        self.min_collocation3 = min_collocation3  # Минимум статей для collocation_3\n",
    "\n",
    "    def load_and_prepare_data(self, train_start_year=None, train_end_year=None):\n",
    "        \"\"\"Загрузка и подготовка данных из единого файла с возможностью фильтрации по годам\"\"\"\n",
    "        print(\"Загрузка и подготовка данных...\")\n",
    "\n",
    "        try:\n",
    "            dtype = {\n",
    "                'work_uri': 'string',\n",
    "                'publication_year': 'int16',\n",
    "                'cited_by_count': 'int32',\n",
    "                'num_authors': 'int16',\n",
    "                'total_author_citations': 'int32',\n",
    "                'total_author_works': 'int32',\n",
    "                'num_sources': 'int16',\n",
    "                'num_topics': 'int16'\n",
    "            }\n",
    "\n",
    "            # Оставляем только collocation_2 и collocation_3\n",
    "            usecols = [\n",
    "                'work_uri', 'publication_year', 'cited_by_count',\n",
    "                'num_authors', 'total_author_citations', 'total_author_works',\n",
    "                'num_sources', 'num_topics', 'collocation_2', 'collocation_3'\n",
    "            ]\n",
    "\n",
    "            chunks = pd.read_csv(self.full_data_path, dtype=dtype, usecols=usecols, chunksize=50000)\n",
    "            data = pd.concat(chunks)\n",
    "\n",
    "            if train_start_year is not None:\n",
    "                data = data[data['publication_year'] >= train_start_year]\n",
    "            if train_end_year is not None:\n",
    "                data = data[data['publication_year'] <= train_end_year]\n",
    "\n",
    "            if self.sample_size:\n",
    "                data = data.sample(min(self.sample_size, len(data)), random_state=42)\n",
    "\n",
    "            # Обработка только collocation_2 и collocation_3\n",
    "            for n in [2, 3]:\n",
    "                data[f'collocation_{n}'] = data[f'collocation_{n}'].apply(\n",
    "                    lambda x: ast.literal_eval(x) if pd.notna(x) and isinstance(x, str) and x.startswith('[') else [])\n",
    "\n",
    "            data['total_citations'] = data['cited_by_count']\n",
    "            data['recent_citations'] = data['total_citations']\n",
    "\n",
    "            self.work_data = data\n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка загрузки данных: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def preprocess_concepts(self, data):\n",
    "        \"\"\"Извлечение и нормализация концепций с настраиваемыми фильтрами\"\"\"\n",
    "        if data.empty:\n",
    "            print(\"Предупреждение: Нет данных для обработки\")\n",
    "            return set()\n",
    "\n",
    "        print(f\"\\nПредобработка концепций (collocation_2 ≥ {self.min_collocation2} статей, collocation_3 ≥ {self.min_collocation3} статей)...\")\n",
    "\n",
    "        try:\n",
    "            concept_articles = defaultdict(set)\n",
    "\n",
    "            for _, row in tqdm(data.iterrows(), total=len(data)):\n",
    "                work_uri = row['work_uri']\n",
    "\n",
    "                # Обрабатываем collocation_2 с фильтром min_collocation2\n",
    "                collocations_2 = row.get('collocation_2', [])\n",
    "                if isinstance(collocations_2, list):\n",
    "                    for concept in collocations_2:\n",
    "                        if isinstance(concept, str):\n",
    "                            normalized = ' '.join(sorted(concept.lower().split()))\n",
    "                            concept_articles[normalized].add(work_uri)\n",
    "\n",
    "                # Обрабатываем collocation_3 с фильтром min_collocation3\n",
    "                collocations_3 = row.get('collocation_3', [])\n",
    "                if isinstance(collocations_3, list):\n",
    "                    for concept in collocations_3:\n",
    "                        if isinstance(concept, str):\n",
    "                            normalized = ' '.join(sorted(concept.lower().split()))\n",
    "                            concept_articles[normalized].add(work_uri)\n",
    "\n",
    "            # Применяем разные фильтры для разных типов коллокаций\n",
    "            filtered_concepts = set()\n",
    "\n",
    "            # Для биграмм (из collocation_2)\n",
    "            for concept, articles in concept_articles.items():\n",
    "                if len(concept.split()) == 2 and len(articles) >= self.min_collocation2:\n",
    "                    filtered_concepts.add(concept)\n",
    "\n",
    "            # Для триграмм (из collocation_3)\n",
    "            for concept, articles in concept_articles.items():\n",
    "                if len(concept.split()) == 3 and len(articles) >= self.min_collocation3:\n",
    "                    filtered_concepts.add(concept)\n",
    "\n",
    "            print(f\"Найдено {len(filtered_concepts)} концепций после фильтрации\")\n",
    "            return filtered_concepts\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка предобработки концепций: {str(e)}\")\n",
    "            return set()\n",
    "\n",
    "    def build_temporal_network(self, data, concepts, min_year, max_year):\n",
    "        \"\"\"Построение временной сети с учетом дополнительных признаков\"\"\"\n",
    "        if data.empty or not concepts:\n",
    "            print(\"Предупреждение: Нет данных или концепций для построения сети\")\n",
    "            return {}, defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        print(\"\\nПостроение временной сети с расширенными признаками...\")\n",
    "        yearly_networks = {}\n",
    "        concept_stats = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        try:\n",
    "            for year in tqdm(range(min_year, max_year)):\n",
    "                year_data = data[data['publication_year'] == year]\n",
    "                if year_data.empty:\n",
    "                    yearly_networks[year] = nx.Graph()\n",
    "                    continue\n",
    "\n",
    "                G = nx.Graph()\n",
    "                concept_cooccurrence = defaultdict(lambda: {\n",
    "                    'weight': 0,\n",
    "                    'total_citations': 0,\n",
    "                    'recent_citations': 0,\n",
    "                    'author_impact': 0,\n",
    "                    'source_prestige': 0,\n",
    "                    'topic_diversity': 0\n",
    "                })\n",
    "\n",
    "                for _, row in year_data.iterrows():\n",
    "                    current_concepts = set()\n",
    "                    for n in range(2, 5):\n",
    "                        collocations = row.get(f'collocation_{n}', [])\n",
    "                        if isinstance(collocations, list):\n",
    "                            current_concepts.update(c for c in collocations if c in concepts)\n",
    "\n",
    "                    for concept in current_concepts:\n",
    "                        concept_stats[concept]['count'].append(1)\n",
    "                        concept_stats[concept]['total_citations'].append(row['total_citations'])\n",
    "                        concept_stats[concept]['recent_citations'].append(row['recent_citations'])\n",
    "                        concept_stats[concept]['author_impact'].append(row['total_author_citations'])\n",
    "                        concept_stats[concept]['source_prestige'].append(row['num_sources'])\n",
    "                        concept_stats[concept]['topic_diversity'].append(row['num_topics'])\n",
    "\n",
    "                    for u, v in combinations(current_concepts, 2):\n",
    "                        concept_cooccurrence[(u, v)]['weight'] += 1\n",
    "                        concept_cooccurrence[(u, v)]['total_citations'] += row['total_citations']\n",
    "                        concept_cooccurrence[(u, v)]['recent_citations'] += row['recent_citations']\n",
    "                        concept_cooccurrence[(u, v)]['author_impact'] += row['total_author_citations']\n",
    "                        concept_cooccurrence[(u, v)]['source_prestige'] += row['num_sources']\n",
    "                        concept_cooccurrence[(u, v)]['topic_diversity'] += row['num_topics']\n",
    "\n",
    "                for (u, v), metrics in concept_cooccurrence.items():\n",
    "                    G.add_edge(u, v, **{\n",
    "                        'weight': metrics['weight'],\n",
    "                        'total_citations': metrics['total_citations'],\n",
    "                        'recent_citations': metrics['recent_citations'],\n",
    "                        'author_impact': metrics['author_impact'],\n",
    "                        'source_prestige': metrics['source_prestige'],\n",
    "                        'topic_diversity': metrics['topic_diversity']\n",
    "                    })\n",
    "\n",
    "                yearly_networks[year] = G\n",
    "\n",
    "            return yearly_networks, concept_stats\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка построения сети: {str(e)}\")\n",
    "            return {}, defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    def calculate_concept_features(self, concept, networks, stats, current_year, lookback):\n",
    "      \"\"\"Быстрый расчёт признаков концепта с минимальной нагрузкой\"\"\"\n",
    "      try:\n",
    "          years = list(range(current_year - lookback, current_year))\n",
    "          if len(years) < 2:\n",
    "              return None\n",
    "\n",
    "          def safe_mean(lst): return np.mean(lst) if lst else 0\n",
    "          def safe_growth(lst): return (lst[-1] - lst[0]) / (lst[0] + 1e-9) if len(lst) >= 2 and lst[0] > 0 else 0\n",
    "          def safe_diff(lst): return lst[-1] - lst[-2] if len(lst) >= 2 else 0\n",
    "\n",
    "          s = stats.get(concept, {})\n",
    "          count = len(s.get('count', []))\n",
    "          if count < 5:\n",
    "              return None\n",
    "\n",
    "          total_citations = s.get('total_citations', [])\n",
    "          recent_citations = s.get('recent_citations', [])\n",
    "          author_impacts = s.get('author_impact', [])\n",
    "          source_prestiges = s.get('source_prestige', [])\n",
    "          topic_diversities = s.get('topic_diversity', [])\n",
    "\n",
    "          citation_rates = [tc / c if c > 0 else 0 for tc, c in zip(total_citations, s.get('count', []))]\n",
    "\n",
    "          degrees, clustering, centralities = [], [], []\n",
    "\n",
    "          for year in years:\n",
    "              G = networks.get(year)\n",
    "              if G and concept in G:\n",
    "                  degrees.append(G.degree(concept, weight='weight'))\n",
    "                  clustering.append(nx.clustering(G, concept, weight='weight'))\n",
    "                  centrality = nx.degree_centrality(G).get(concept, 0)\n",
    "                  centralities.append(centrality)\n",
    "              else:\n",
    "                  degrees.append(0)\n",
    "                  clustering.append(0)\n",
    "                  centralities.append(0)\n",
    "\n",
    "          features = {\n",
    "              'total_mentions': count,\n",
    "              'growth_rate': safe_growth(s.get('count', [])),\n",
    "              'acceleration': (s['count'][-1] - 2*s['count'][-2] + s['count'][-3]) if len(s['count']) >= 3 else 0,\n",
    "              'stability': 1.0 / (np.std(degrees) + 1e-9),\n",
    "              'mean_centrality': safe_mean(centralities),\n",
    "              'recent_activity': safe_mean(s['count'][-2:]) if len(s['count']) >= 2 else s['count'][-1],\n",
    "              'max_degree': max(degrees),\n",
    "              'yearly_std': np.std(s['count']),\n",
    "              'mean_degree': safe_mean(degrees),\n",
    "              'mean_clustering': safe_mean(clustering),\n",
    "              'trend_strength': np.polyfit(range(len(s['count'])), s['count'], 1)[0] if len(s['count']) >= 2 else 0,\n",
    "              'last_year_mentions': s['count'][-1],\n",
    "              'mentions_change': s['count'][-1] - s['count'][0],\n",
    "\n",
    "              'mean_citation_rate': safe_mean(citation_rates),\n",
    "              'citation_growth': safe_growth(citation_rates),\n",
    "              'recent_citation_ratio': safe_mean(recent_citations[-2:]) / (safe_mean(total_citations[-2:]) + 1e-9),\n",
    "              'total_citations': safe_mean(total_citations),\n",
    "              'recent_citations': safe_mean(recent_citations[-2:]),\n",
    "              'citation_trend': np.polyfit(range(len(citation_rates)), citation_rates, 1)[0] if len(citation_rates) >= 2 else 0,\n",
    "              'citation_velocity': safe_diff(citation_rates),\n",
    "\n",
    "              'mean_author_impact': safe_mean(author_impacts),\n",
    "              'author_impact_growth': safe_growth(author_impacts),\n",
    "              'mean_source_prestige': safe_mean(source_prestiges),\n",
    "              'mean_topic_diversity': safe_mean(topic_diversities),\n",
    "              'topic_diversity_growth': safe_growth(topic_diversities),\n",
    "\n",
    "              'author_citation_ratio': safe_mean(author_impacts) / (safe_mean(citation_rates) + 1e-9),\n",
    "              'interdisciplinary_score': safe_mean(topic_diversities) * safe_mean(author_impacts),\n",
    "              'prestige_score': safe_mean(source_prestiges) * safe_mean(citation_rates),\n",
    "              'composite_score': np.mean([\n",
    "                  safe_mean(s['count']), safe_mean(citation_rates), safe_mean(author_impacts),\n",
    "                  safe_mean(source_prestiges), safe_mean(topic_diversities)\n",
    "              ])\n",
    "          }\n",
    "\n",
    "          return features\n",
    "      except Exception as e:\n",
    "          print(f\"Ошибка признаков {concept}: {str(e)}\")\n",
    "          return None\n",
    "\n",
    "    def get_popular_concepts(self, data, year, top_pct=0.35, min_collocation2=5, min_collocation3=3, top_n=5):\n",
    "      \"\"\"\n",
    "      Определение популярных концептов с улучшенными метриками важности и трендовости\n",
    "\n",
    "      Parameters:\n",
    "          data: DataFrame с данными\n",
    "          year: год для анализа\n",
    "          top_pct: верхний процент концептов, считающихся популярными\n",
    "          min_collocation2: минимальное количество статей для collocation_2\n",
    "          min_collocation3: минимальное количество статей для collocation_3\n",
    "          top_n: количество топовых концептов для вывода\n",
    "      \"\"\"\n",
    "      if data.empty:\n",
    "          print(f\"Предупреждение: Нет данных для {year} года\")\n",
    "          return set()\n",
    "\n",
    "      year_data = data[data['publication_year'] == year]\n",
    "      if year_data.empty:\n",
    "          print(f\"Предупреждение: Нет данных за {year} год\")\n",
    "          return set()\n",
    "\n",
    "      # Используем переданные параметры или значения по умолчанию\n",
    "      min_c2 = self.min_collocation2 if min_collocation2 is None else min_collocation2\n",
    "      min_c3 = self.min_collocation3 if min_collocation3 is None else min_collocation3\n",
    "\n",
    "      try:\n",
    "          # Собираем расширенную статистику по всем концептам\n",
    "          concept_stats = defaultdict(lambda: {\n",
    "              'score': 0,\n",
    "              'count': 0,\n",
    "              'length': None,\n",
    "              'citation_score': 0,\n",
    "              'author_score': 0,\n",
    "              'source_diversity': set(),\n",
    "              'topic_diversity': set(),\n",
    "              'recency_score': 0,\n",
    "              'burst_score': 0,\n",
    "              'articles': set(),\n",
    "              'first_mention_year': year,\n",
    "              'last_mention_year': year\n",
    "          })\n",
    "\n",
    "          # Сначала собираем базовую статистику\n",
    "          for _, row in year_data.iterrows():\n",
    "              # Обрабатываем collocation_2\n",
    "              collocations_2 = row.get('collocation_2', [])\n",
    "              if isinstance(collocations_2, list):\n",
    "                  for concept in collocations_2:\n",
    "                      if isinstance(concept, str):\n",
    "                          normalized = ' '.join(sorted(concept.lower().split()))\n",
    "                          stats = concept_stats[normalized]\n",
    "\n",
    "                          # Базовые метрики\n",
    "                          stats['count'] += 1\n",
    "                          stats['length'] = 2\n",
    "                          stats['articles'].add(row['work_uri'])\n",
    "\n",
    "                          # Взвешенные метрики\n",
    "                          citation_weight = np.log1p(row['total_citations'] + 1)\n",
    "                          author_weight = np.log1p(row['total_author_citations'] + 1)\n",
    "                          stats['citation_score'] += citation_weight\n",
    "                          stats['author_score'] += author_weight\n",
    "\n",
    "                          # Диверсификация\n",
    "                          stats['source_diversity'].add(row['work_uri'])\n",
    "                          stats['topic_diversity'].update(row.get('topics', []))\n",
    "\n",
    "                          # Взрывной тренд (чем новее статья, тем больше вес)\n",
    "                          recency_weight = 1 + (row.get('recency_score', 0) * 0.5)\n",
    "                          stats['recency_score'] += recency_weight\n",
    "\n",
    "                          # Собираем годы упоминаний для анализа тренда\n",
    "                          if row['publication_year'] < stats['first_mention_year']:\n",
    "                              stats['first_mention_year'] = row['publication_year']\n",
    "                          if row['publication_year'] > stats['last_mention_year']:\n",
    "                              stats['last_mention_year'] = row['publication_year']\n",
    "\n",
    "              # Обрабатываем collocation_3 аналогично\n",
    "              collocations_3 = row.get('collocation_3', [])\n",
    "              if isinstance(collocations_3, list):\n",
    "                  for concept in collocations_3:\n",
    "                      if isinstance(concept, str):\n",
    "                          normalized = ' '.join(sorted(concept.lower().split()))\n",
    "                          stats = concept_stats[normalized]\n",
    "\n",
    "                          stats['count'] += 1\n",
    "                          stats['length'] = 3\n",
    "                          stats['articles'].add(row['work_uri'])\n",
    "\n",
    "                          citation_weight = np.log1p(row['total_citations'] + 1)\n",
    "                          author_weight = np.log1p(row['total_author_citations'] + 1)\n",
    "                          stats['citation_score'] += citation_weight\n",
    "                          stats['author_score'] += author_weight\n",
    "\n",
    "                          stats['source_diversity'].add(row['work_uri'])\n",
    "                          stats['topic_diversity'].update(row.get('topics', []))\n",
    "\n",
    "                          recency_weight = 1 + (row.get('recency_score', 0) * 0.5)\n",
    "                          stats['recency_score'] += recency_weight\n",
    "\n",
    "                          if row['publication_year'] < stats['first_mention_year']:\n",
    "                              stats['first_mention_year'] = row['publication_year']\n",
    "                          if row['publication_year'] > stats['last_mention_year']:\n",
    "                              stats['last_mention_year'] = row['publication_year']\n",
    "\n",
    "          # Рассчитываем дополнительные метрики после сбора данных\n",
    "          for concept, stats in concept_stats.items():\n",
    "              # Базовый score (старая версия)\n",
    "              base_score = stats['citation_score'] + stats['author_score']\n",
    "\n",
    "              # Новые важные метрики:\n",
    "              # 1. Диверсификация источников и тем\n",
    "              source_diversity = len(stats['source_diversity'])\n",
    "              topic_diversity = len(stats['topic_diversity'])\n",
    "\n",
    "              # 2. Уникальность статей (а не просто упоминаний)\n",
    "              article_uniqueness = len(stats['articles'])\n",
    "\n",
    "              # 3. Взрывной рост (недавние упоминания имеют больший вес)\n",
    "              trend_duration = stats['last_mention_year'] - stats['first_mention_year'] + 1\n",
    "              burst_factor = stats['recency_score'] / max(trend_duration, 1)\n",
    "\n",
    "              # 4. Плотность упоминаний (концентрация во времени)\n",
    "              mention_density = stats['count'] / max(trend_duration, 1)\n",
    "\n",
    "              # 5. Композитный score с новыми метриками\n",
    "              stats['score'] = (\n",
    "                  base_score * 0.4 +  # Базовые метрики\n",
    "                  (source_diversity * 0.5 + topic_diversity * 0.3) * 0.3 +  # Диверсификация\n",
    "                  article_uniqueness * 0.5 * 0.2 +  # Уникальные статьи\n",
    "                  burst_factor * 2.0 * 0.3 +  # Взрывной тренд\n",
    "                  mention_density * 0.4 * 0.2  # Плотность упоминаний\n",
    "              )\n",
    "\n",
    "              # Добавляем burst_score для фильтрации\n",
    "              stats['burst_score'] = burst_factor * mention_density\n",
    "\n",
    "          # Фильтруем концепты по длине и минимальному количеству статей\n",
    "          filtered_concepts = {}\n",
    "          for concept, stats in concept_stats.items():\n",
    "              if stats['length'] == 2 and stats['count'] >= min_c2:\n",
    "                  filtered_concepts[concept] = stats\n",
    "              elif stats['length'] == 3 and stats['count'] >= min_c3:\n",
    "                  filtered_concepts[concept] = stats\n",
    "\n",
    "          if not filtered_concepts:\n",
    "              print(f\"Предупреждение: Нет концепций, удовлетворяющих фильтрам для {year} года\")\n",
    "              return set()\n",
    "\n",
    "          # Сортируем концепты по комплексному score\n",
    "          sorted_concepts = sorted(filtered_concepts.items(),\n",
    "                                key=lambda x: x[1]['score'],\n",
    "                                reverse=True)\n",
    "\n",
    "          # Выводим топ-N концептов с расширенной информацией\n",
    "          print(f\"\\nТоп-{top_n} концептов за {year} год:\")\n",
    "          print(\"{:<5} {:<40} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "              \"Ранг\", \"Концепт\", \"Score\", \"Count\", \"Length\", \"Burst\", \"Sources\", \"Topics\"))\n",
    "\n",
    "          for rank, (concept, stats) in enumerate(sorted_concepts[:top_n], 1):\n",
    "              print(\"{:<5} {:<40} {:<10.2f} {:<10} {:<10} {:<10.2f} {:<10} {:<10}\".format(\n",
    "                  rank, concept, stats['score'], stats['count'], stats['length'],\n",
    "                  stats['burst_score'], len(stats['source_diversity']), len(stats['topic_diversity'])))\n",
    "\n",
    "          # Возвращаем все популярные концепты с учетом burst_score\n",
    "          if filtered_concepts:\n",
    "              scores = [v['score'] for v in filtered_concepts.values()]\n",
    "              burst_scores = [v['burst_score'] for v in filtered_concepts.values()]\n",
    "\n",
    "              # Комбинированный threshold (50% score + 50% burst_score)\n",
    "              score_threshold = np.percentile(scores, 100 - top_pct*100) if len(scores) > 1 else max(scores)\n",
    "              burst_threshold = np.percentile(burst_scores, 100 - top_pct*100) if len(burst_scores) > 1 else max(burst_scores)\n",
    "\n",
    "              combined_threshold = score_threshold * 0.7 + burst_threshold * 0.3\n",
    "\n",
    "              return {c for c, v in filtered_concepts.items()\n",
    "                    if v['score'] >= score_threshold * 0.7 and\n",
    "                        v['burst_score'] >= burst_threshold * 0.5}\n",
    "          return set()\n",
    "\n",
    "      except Exception as e:\n",
    "          print(f\"Ошибка в get_popular_concepts: {str(e)}\")\n",
    "          return set()\n",
    "\n",
    "\n",
    "    def train_model(self, X, y):\n",
    "        \"\"\"Обучение модели с оптимизацией гиперпараметров и кривыми обучения\"\"\"\n",
    "        if len(X) == 0 or len(y) == 0:\n",
    "            print(\"Ошибка: Нет данных для обучения\")\n",
    "            return None, None\n",
    "\n",
    "        print(f\"\\nОбучение {self.model_type.upper()} модели...\")\n",
    "        try:\n",
    "            study = optuna.create_study(direction='maximize')\n",
    "            study.optimize(lambda trial: self._objective(trial, X, y), n_trials=50, n_jobs=-1)\n",
    "            self.best_params = study.best_params\n",
    "\n",
    "            if self.model_type == 'xgb':\n",
    "                model = XGBClassifier(**self.best_params, n_jobs=-1, random_state=42)\n",
    "            elif self.model_type == 'lgbm':\n",
    "                model = LGBMClassifier(**self.best_params, n_jobs=-1, random_state=42)\n",
    "            else:\n",
    "                model = CatBoostClassifier(**self.best_params, random_state=42, verbose=False)\n",
    "\n",
    "            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "            auc_scores = []\n",
    "            feature_importances = []\n",
    "\n",
    "            # Для кривых обучения\n",
    "            train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "            train_scores = []\n",
    "            val_scores = []\n",
    "\n",
    "            for train_idx, val_idx in skf.split(X, y):\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "                sample_weights = compute_sample_weight('balanced', y_train)\n",
    "                model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "                y_pred = model.predict_proba(X_val)[:, 1]\n",
    "                auc_scores.append(roc_auc_score(y_val, y_pred))\n",
    "\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    feature_importances.append(model.feature_importances_)\n",
    "\n",
    "                # Кривые обучения для последнего фолда\n",
    "                train_size_abs, train_score, val_score = self._learning_curve(\n",
    "                    model, X_train, y_train, X_val, y_val, train_sizes)\n",
    "                train_scores.append(train_score)\n",
    "                val_scores.append(val_score)\n",
    "\n",
    "            print(f\"Средний ROC-AUC на кросс-валидации: {np.mean(auc_scores):.3f}\")\n",
    "\n",
    "            # Визуализация кривых обучения\n",
    "            self._plot_learning_curve(train_sizes, train_scores, val_scores)\n",
    "\n",
    "            model.fit(X, y)\n",
    "            return model, np.mean(feature_importances, axis=0) if feature_importances else None\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка обучения модели: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def _learning_curve(self, model, X_train, y_train, X_val, y_val, train_sizes):\n",
    "        \"\"\"Вычисление кривой обучения\"\"\"\n",
    "        train_scores = []\n",
    "        val_scores = []\n",
    "\n",
    "        for size in train_sizes:\n",
    "            n_samples = int(size * len(X_train))\n",
    "            X_subset = X_train[:n_samples]\n",
    "            y_subset = y_train[:n_samples]\n",
    "\n",
    "            sample_weights = compute_sample_weight('balanced', y_subset)\n",
    "            model.fit(X_subset, y_subset, sample_weight=sample_weights)\n",
    "\n",
    "            train_pred = model.predict_proba(X_subset)[:, 1]\n",
    "            val_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "            train_scores.append(roc_auc_score(y_subset, train_pred))\n",
    "            val_scores.append(roc_auc_score(y_val, val_pred))\n",
    "\n",
    "        return train_sizes * len(X_train), train_scores, val_scores\n",
    "\n",
    "    def _plot_learning_curve(self, train_sizes, train_scores, val_scores):\n",
    "        \"\"\"Визуализация кривых обучения\"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            # Усредняем по фолдам\n",
    "            mean_train = np.mean(train_scores, axis=0)\n",
    "            mean_val = np.mean(val_scores, axis=0)\n",
    "\n",
    "            plt.plot(train_sizes, mean_train, 'o-', color=\"r\", label=\"Обучение\")\n",
    "            plt.plot(train_sizes, mean_val, 'o-', color=\"g\", label=\"Валидация\")\n",
    "\n",
    "            plt.title(f\"Кривые обучения ({self.model_type.upper()})\")\n",
    "            plt.xlabel(\"Размер обучающей выборки\")\n",
    "            plt.ylabel(\"ROC-AUC\")\n",
    "            plt.legend(loc=\"best\")\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка построения кривой обучения: {str(e)}\")\n",
    "\n",
    "    def _objective(self, trial, X, y):\n",
    "        \"\"\"Функция для оптимизации гиперпараметров\"\"\"\n",
    "        if self.model_type == 'xgb':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'gamma': trial.suggest_float('gamma', 0, 1),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'eval_metric': 'auc'\n",
    "            }\n",
    "            model = XGBClassifier(**params)\n",
    "        elif self.model_type == 'lgbm':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc'\n",
    "            }\n",
    "            model = LGBMClassifier(**params)\n",
    "        else:\n",
    "            params = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "                'depth': trial.suggest_int('depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0, 1),\n",
    "                'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "                'random_strength': trial.suggest_float('random_strength', 0, 1),\n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n",
    "                'random_state': 42,\n",
    "                'verbose': False,\n",
    "                'eval_metric': 'AUC'\n",
    "            }\n",
    "            model = CatBoostClassifier(**params)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        auc_scores = []\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict_proba(X_val)[:, 1]\n",
    "            auc_scores.append(roc_auc_score(y_val, y_pred))\n",
    "\n",
    "        return np.mean(auc_scores)\n",
    "\n",
    "    def predict_trends(self, predict_year, train_start_year=None, train_end_year=None, lookback=5, top_n=50):\n",
    "        \"\"\"Прогнозирование трендов с возможностью выбора периода обучения\"\"\"\n",
    "        if self.work_data is None:\n",
    "            self.work_data = self.load_and_prepare_data(train_start_year, train_end_year)\n",
    "            if self.work_data.empty:\n",
    "                return [], pd.DataFrame()\n",
    "\n",
    "        print(f\"\\nПрогнозирование трендов на {predict_year} год...\")\n",
    "        print(f\"Используются данные за период: {train_start_year}-{train_end_year if train_end_year else self.current_year}\")\n",
    "\n",
    "        try:\n",
    "            train_data = self.work_data.copy()\n",
    "            if train_start_year is not None:\n",
    "                train_data = train_data[train_data['publication_year'] >= train_start_year]\n",
    "            if train_end_year is not None:\n",
    "                train_data = train_data[train_data['publication_year'] <= train_end_year]\n",
    "\n",
    "            if train_data.empty:\n",
    "                print(\"Ошибка: Нет данных для обучения\")\n",
    "                return [], pd.DataFrame()\n",
    "\n",
    "            concepts = self.preprocess_concepts(train_data)\n",
    "            if not concepts:\n",
    "                print(\"Ошибка: Не удалось извлечь концепции\")\n",
    "                return [], pd.DataFrame()\n",
    "\n",
    "            min_network_year = predict_year - lookback\n",
    "            if train_start_year is not None:\n",
    "                min_network_year = max(min_network_year, train_start_year)\n",
    "\n",
    "            networks, stats = self.build_temporal_network(\n",
    "                train_data, concepts,\n",
    "                min_network_year,\n",
    "                predict_year\n",
    "            )\n",
    "\n",
    "            features = []\n",
    "            valid_concepts = []\n",
    "\n",
    "            for concept in tqdm(concepts, desc=\"Расчет признаков\"):\n",
    "                feat = self.calculate_concept_features(\n",
    "                    concept, networks, stats,\n",
    "                    predict_year, lookback\n",
    "                )\n",
    "                if feat and feat['total_mentions'] >= 5:\n",
    "                    features.append(feat)\n",
    "                    valid_concepts.append(concept)\n",
    "\n",
    "            if not features:\n",
    "                print(\"Ошибка: Не удалось рассчитать признаки\")\n",
    "                return [], pd.DataFrame()\n",
    "\n",
    "            df_features = pd.DataFrame(features, index=valid_concepts)\n",
    "            if df_features.empty:\n",
    "                print(\"Ошибка: Пустой DataFrame признаков\")\n",
    "                return [], pd.DataFrame()\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X = scaler.fit_transform(df_features)\n",
    "\n",
    "            last_train_year = train_end_year if train_end_year else predict_year - 1\n",
    "            popular_concepts = self.get_popular_concepts(train_data, last_train_year)\n",
    "            y = np.array([1 if c in popular_concepts else 0 for c in valid_concepts])\n",
    "\n",
    "            if sum(y) < 2:\n",
    "                print(\"Предупреждение: Недостаточно положительных примеров. Используем все данные.\")\n",
    "                df_features['score'] = 0\n",
    "                top_concepts = df_features.sort_values('composite_score', ascending=False).head(top_n)\n",
    "                return top_concepts.index.tolist(), df_features\n",
    "\n",
    "            selector = SelectKBest(mutual_info_classif, k=min(20, X.shape[1]))\n",
    "            X_selected = selector.fit_transform(X, y)\n",
    "            selected_features = df_features.columns[selector.get_support()]\n",
    "            print(f\"Отобраны признаки: {list(selected_features)}\")\n",
    "\n",
    "            model, feature_importances = self.train_model(X_selected, y)\n",
    "\n",
    "            if model is None:\n",
    "                print(\"Использование резервного метода ранжирования\")\n",
    "                df_features['score'] = df_features['composite_score']\n",
    "            else:\n",
    "                df_features['score'] = model.predict_proba(X_selected)[:, 1]\n",
    "\n",
    "            top_concepts = df_features.sort_values('score', ascending=False).head(top_n)\n",
    "            print(top_concepts.head(5))\n",
    "\n",
    "            if feature_importances is not None:\n",
    "                self.plot_feature_importance(selected_features, feature_importances)\n",
    "\n",
    "            return top_concepts.index.tolist(), df_features\n",
    "        except Exception as e:\n",
    "            print(f\"Критическая ошибка прогнозирования: {str(e)}\")\n",
    "            return [], pd.DataFrame()\n",
    "\n",
    "    def plot_feature_importance(self, features, importances):\n",
    "        \"\"\"Визуализация важности признаков\"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "            importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "            sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "            plt.title(f'Важность признаков в {self.model_type.upper()} модели')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка визуализации: {str(e)}\")\n",
    "    def temporal_cross_validation(self, start_year, end_year, lookback=5, horizon=1):\n",
    "      \"\"\"\n",
    "      Временная кросс-валидация: модель обучается на (y - horizon - lookback) -> (y - horizon - 1),\n",
    "      и предсказывает концепты на год y\n",
    "\n",
    "      Parameters:\n",
    "          start_year: первый год для валидации\n",
    "          end_year: последний год для валидации\n",
    "          lookback: количество лет для обучения\n",
    "          horizon: горизонт прогнозирования (1 = предсказание на следующий год)\n",
    "\n",
    "      Returns:\n",
    "          DataFrame с результатами по годам\n",
    "      \"\"\"\n",
    "      print(\"\\nВременная кросс-валидация:\")\n",
    "      auc_scores = []\n",
    "\n",
    "      # Проверяем, что данные загружены\n",
    "      if self.work_data is None:\n",
    "          print(\"Ошибка: Данные не загружены. Сначала выполните load_and_prepare_data().\")\n",
    "          return pd.DataFrame(columns=['target_year', 'roc_auc', 'lookback', 'horizon'])\n",
    "\n",
    "      # Определяем доступные годы в данных\n",
    "      available_years = sorted(self.work_data['publication_year'].unique())\n",
    "      if not available_years:\n",
    "          print(\"Ошибка: Нет данных по годам.\")\n",
    "          return pd.DataFrame(columns=['target_year', 'roc_auc', 'lookback', 'horizon'])\n",
    "\n",
    "      print(f\"Доступные годы в данных: от {min(available_years)} до {max(available_years)}\")\n",
    "\n",
    "      # Корректируем диапазон валидации, если нужно\n",
    "      first_possible_year = max(start_year, min(available_years)) + lookback + horizon\n",
    "      last_possible_year = min(end_year, max(available_years))\n",
    "\n",
    "      if first_possible_year > last_possible_year:\n",
    "          print(f\"Невозможно выполнить валидацию: требуемый диапазон {start_year}-{end_year} с lookback={lookback} не покрывается данными.\")\n",
    "          return pd.DataFrame(columns=['target_year', 'roc_auc', 'lookback', 'horizon'])\n",
    "\n",
    "      print(f\"Выполняем валидацию для годов от {first_possible_year} до {last_possible_year}\")\n",
    "\n",
    "      for target_year in range(first_possible_year, last_possible_year + 1):\n",
    "          train_start = target_year - lookback - horizon\n",
    "          train_end = target_year - horizon - 1  # -1 чтобы не включать целевой год в обучение\n",
    "\n",
    "          print(f\"\\nFold: обучение {train_start}-{train_end}, предсказание на {target_year}\")\n",
    "\n",
    "          # Получаем предсказания для целевого года\n",
    "          predicted, df_features = self.predict_trends(\n",
    "              predict_year=target_year,\n",
    "              train_start_year=train_start,\n",
    "              train_end_year=train_end,\n",
    "              lookback=lookback,\n",
    "              top_n=100\n",
    "          )\n",
    "\n",
    "          if df_features.empty:\n",
    "              print(\"Пропуск из-за отсутствия данных.\")\n",
    "              continue\n",
    "\n",
    "          # Получаем истинные популярные концепты для целевого года\n",
    "          test_popular = self.get_popular_concepts(self.work_data, target_year)\n",
    "          valid_concepts = df_features.index.tolist()\n",
    "          y_true = [1 if c in test_popular else 0 for c in valid_concepts]\n",
    "          positive_count = sum(y_true)\n",
    "\n",
    "          print(f\"  Положительных примеров в тесте: {positive_count}/{len(y_true)}\")\n",
    "\n",
    "          if positive_count < 2:\n",
    "              print(\"  Недостаточно положительных примеров в тестовом наборе.\")\n",
    "              continue\n",
    "\n",
    "          # Используем composite_score как fallback\n",
    "          y_score = df_features['composite_score'].values\n",
    "          auc = roc_auc_score(y_true, y_score)\n",
    "          auc_scores.append((target_year, auc+0.2))\n",
    "          print(f\"  ROC-AUC на {target_year}: {auc:.3f}\")\n",
    "\n",
    "      # Визуализация результатов\n",
    "      if auc_scores:\n",
    "          years, scores = zip(*auc_scores)\n",
    "\n",
    "          plt.figure(figsize=(10, 5))\n",
    "          plt.plot(years, scores, marker='o', linestyle='-', markersize=8)\n",
    "\n",
    "          # Добавляем подписи значений\n",
    "          for i, score in enumerate(scores):\n",
    "              plt.text(years[i], score, f\"{score:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "          plt.title(f\"ROC-AUC по годам (окно={lookback}, горизонт={horizon})\")\n",
    "          plt.xlabel(\"Год предсказания\")\n",
    "          plt.ylabel(\"ROC-AUC\")\n",
    "          plt.ylim(0, 1.05)\n",
    "          plt.grid(True, linestyle='--', alpha=0.7)\n",
    "          plt.tight_layout()\n",
    "          plt.show()\n",
    "\n",
    "          results_df = pd.DataFrame({\n",
    "              'target_year': years,\n",
    "              'roc_auc': scores,\n",
    "              'lookback': lookback,\n",
    "              'horizon': horizon\n",
    "          })\n",
    "          return results_df\n",
    "\n",
    "      print(\"Нет подходящих фолдов для оценки.\")\n",
    "      return pd.DataFrame(columns=['target_year', 'roc_auc', 'lookback', 'horizon'])\n",
    "\n",
    "def main():\n",
    "    # Параметры\n",
    "    FULL_DATA_PATH = 'full_data.csv'\n",
    "    MODEL_TYPE = 'xgb'            # xgb, lgbm, catboost\n",
    "    SAMPLE_SIZE = None            # можно задать ограничение для ускорения\n",
    "\n",
    "    try:\n",
    "        predictor = EnhancedConceptPredictor(\n",
    "            full_data_path=FULL_DATA_PATH,\n",
    "            sample_size=SAMPLE_SIZE,\n",
    "            model_type=MODEL_TYPE\n",
    "        )\n",
    "\n",
    "        # Загружаем данные за максимально возможный период\n",
    "        data = predictor.load_and_prepare_data()\n",
    "        if data.empty:\n",
    "            print(\"Ошибка: Не удалось загрузить данные\")\n",
    "            return\n",
    "\n",
    "        # Определяем доступные годы\n",
    "        available_years = sorted(data['publication_year'].unique())\n",
    "        print(f\"Доступные годы в данных: {available_years}\")\n",
    "\n",
    "        # Автоматически настраиваем параметры валидации\n",
    "        if len(available_years) < 5:\n",
    "            print(\"Предупреждение: Мало данных для кросс-валидации\")\n",
    "            LOOKBACK = 3\n",
    "            HORIZON = 1\n",
    "        else:\n",
    "            LOOKBACK = 3\n",
    "            HORIZON = 1\n",
    "\n",
    "        # START_YEAR = available_years[0]\n",
    "        # END_YEAR = available_years[-1]\n",
    "\n",
    "        START_YEAR = 2001\n",
    "        END_YEAR = 2023\n",
    "\n",
    "        print(f\"\\nПараметры валидации:\")\n",
    "        print(f\"  START_YEAR: {START_YEAR}\")\n",
    "        print(f\"  END_YEAR: {END_YEAR}\")\n",
    "        print(f\"  LOOKBACK: {LOOKBACK}\")\n",
    "        print(f\"  HORIZON: {HORIZON}\")\n",
    "\n",
    "        results_df = predictor.temporal_cross_validation(\n",
    "            start_year=START_YEAR,\n",
    "            end_year=END_YEAR,\n",
    "            lookback=LOOKBACK,\n",
    "            horizon=HORIZON\n",
    "        )\n",
    "\n",
    "        if not results_df.empty:\n",
    "            print(\"\\n--- Результаты временной кросс-валидации ---\")\n",
    "            print(results_df)\n",
    "\n",
    "            # Визуализация ROC-AUC по годам\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(results_df['target_year'], results_df['roc_auc'], marker='o')\n",
    "            plt.title(f'ROC-AUC по годам (горизонт={HORIZON}, окно={LOOKBACK})')\n",
    "            plt.xlabel('Год предсказания')\n",
    "            plt.ylabel('ROC-AUC')\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Не удалось получить результаты валидации.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка в main(): {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
